# 1.4 - Data Lakehouse vs Traditional DWH: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä

## –í–≤–µ–¥–µ–Ω–∏–µ

Data Lakehouse - —ç—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –ª—É—á—à–∏–µ —á–µ—Ä—Ç—ã Data Lake –∏ Data Warehouse. –†–∞—Å—Å–º–æ—Ç—Ä–∏–º –∫–ª—é—á–µ–≤—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.

## üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä

| –ê—Å–ø–µ–∫—Ç                 | Traditional DWH   | Data Lake      | Data Lakehouse         |
| ---------------------- | ----------------- | -------------- | ---------------------- |
| **–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö**         | –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ | –í—Å–µ —Ç–∏–ø—ã       | –í—Å–µ —Ç–∏–ø—ã               |
| **–°—Ö–µ–º–∞**              | Schema-on-Write   | Schema-on-Read | Schema-on-Read & Write |
| **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** | üü¢ –í—ã—Å–æ–∫–∞—è (SQL)  | üî¥ –ù–∏–∑–∫–∞—è      | üü¢ –í—ã—Å–æ–∫–∞—è (SQL+)      |
| **–ì–∏–±–∫–æ—Å—Ç—å**           | üî¥ –ù–∏–∑–∫–∞—è         | üü¢ –í—ã—Å–æ–∫–∞—è     | üü¢ –í—ã—Å–æ–∫–∞—è             |
| **–°—Ç–æ–∏–º–æ—Å—Ç—å**          | üî¥ –í—ã—Å–æ–∫–∞—è        | üü¢ –ù–∏–∑–∫–∞—è      | üü° –°—Ä–µ–¥–Ω—è—è             |
| **ACID —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏**    | üü¢ –ü–æ–ª–Ω—ã–µ         | üî¥ –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç | üü¢ –ü–æ–ª–Ω—ã–µ              |
| **ML –ø–æ–¥–¥–µ—Ä–∂–∫–∞**       | üî¥ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è   | üü¢ –ü–æ–ª–Ω–∞—è      | üü¢ –ü–æ–ª–Ω–∞—è              |

---

## üèõÔ∏è **Traditional Data Warehouse**

### **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**

```bash
–ò—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Üí ETL ‚Üí DWH (Structured) ‚Üí BI Tools
    ‚Üì
–ñ–µ—Å—Ç–∫–∞—è —Å—Ö–µ–º–∞, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–¥ SQL
```

### **–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫**

- **Cloud:** Snowflake, Redshift, BigQuery, Synapse
- **On-Prem:** Teradata, Oracle Exadata, Vertica
- **ETL:** Informatica, Talend, SSIS

### **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**

```sql
-- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è SQL-–∞–Ω–∞–ª–∏—Ç–∏–∫–∏
SELECT
    customer_segment,
    AVG(order_value) as avg_order,
    COUNT(*) as order_count
FROM fact_orders fo
JOIN dim_customers dc ON fo.customer_id = dc.customer_id
WHERE order_date >= '2024-01-01'
GROUP BY customer_segment
ORDER BY avg_order DESC;
```

### **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è**

- ‚ùå –¢–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
- ‚ùå –í—ã—Å–æ–∫–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏—è
- ‚ùå –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ö–µ–º—ã
- ‚ùå –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ ML/AI

---

## üèûÔ∏è **Data Lake**

### **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**

```bash
–í—Å–µ –¥–∞–Ω–Ω—ã–µ ‚Üí Data Lake (Raw) ‚Üí Processing ‚Üí Analytics
    ‚Üì
–ì–∏–±–∫–∞—è —Å—Ö–µ–º–∞, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—é–±—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
```

### **–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫**

- **Storage:** AWS S3, Azure Blob, Google Cloud Storage
- **Processing:** Spark, Hadoop, Databricks
- **Format:** Parquet, AVRO, JSON, ORC

### **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**

```csharp
// ML Pipeline
var rawImages = spark.Read().Format("binaryFile").Load("s3://lake/raw/images/");
var processedData = mlPipeline.Transform(rawImages);

// Log Analysis
var logsDf = spark.Read().Json("s3://lake/raw/logs/");
var analytics = logsDf.Filter("level = 'ERROR'").GroupBy("service").Count();
```

### **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è**

- ‚ùå –ù–µ—Ç ACID —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
- ‚ùå –°–ª–∞–±–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è BI
- ‚ùå –°–ª–æ–∂–Ω–æ—Å—Ç—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö
- ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –µ–¥–∏–Ω–æ–≥–æ –º–µ—Ç–∞–¥–∞–Ω–Ω–æ–≥–æ

---

## üè† **Data Lakehouse**

### **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**

```bash
–í—Å–µ –¥–∞–Ω–Ω—ã–µ ‚Üí Lakehouse (Open Format) ‚Üí BI & AI
    ‚Üì
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–∏–±–∫–æ—Å—Ç—å Lake –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å DWH
```

### **–ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã**

```sql
Storage Layer (Object Store)
‚îú‚îÄ‚îÄ Delta Lake / Iceberg / Hudi
‚îú‚îÄ‚îÄ ACID Transactions
‚îî‚îÄ‚îÄ Versioning

Metadata Layer
‚îú‚îÄ‚îÄ Unified Catalog
‚îú‚îÄ‚îÄ Schema Evolution
‚îî‚îÄ‚îÄ Data Governance

Compute Layer
‚îú‚îÄ‚îÄ SQL Analytics Engine
‚îú‚îÄ‚îÄ ML Frameworks
‚îî‚îÄ‚îÄ Streaming Processing
```

### **–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫**

- **Formats:** Delta Lake, Apache Iceberg, Apache Hudi
- **Platforms:** Databricks Lakehouse, Snowflake, BigLake
- **Tools:** Spark, dbt, MLflow, Presto

### **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**

```sql
-- SQL –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ–≤–µ—Ä—Ö –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
-- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
SELECT user_id, SUM(amount) as total_spent
FROM delta.`s3://lakehouse/sales/`
WHERE transaction_date > '2024-01-01'
GROUP BY user_id;

-- –ü–æ–ª—É—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
SELECT JSON_EXTRACT(log_data, '$.user_id') as user_id,
       COUNT(*) as error_count
FROM delta.`s3://lakehouse/logs/`
WHERE JSON_EXTRACT(log_data, '$.level') = 'ERROR'
GROUP BY user_id;
```

```csharp
// ML –ø–æ–≤–µ—Ä—Ö —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö
using Microsoft.Spark.ML;
using Delta.Tables;

// –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML
var trainingData = spark.Read().Format("delta").Load("s3://lakehouse/sales/");
var mlModel = pipeline.Fit(trainingData);

// –ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ
predictions.Write().Format("delta").Save("s3://lakehouse/predictions/");
```

---

## üéØ **–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã**

### **–í—ã–±–∏—Ä–∞–π—Ç–µ TRADITIONAL DWH –µ—Å–ª–∏:**

- ‚úÖ –ó—Ä–µ–ª—ã–µ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å—ã —Å–æ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º–∏ —Å—Ö–µ–º–∞–º–∏
- ‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ SQL-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞
- ‚úÖ –°—Ç—Ä–æ–≥–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ compliance –∏ governance
- ‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å –≤ ML/AI

### **–í—ã–±–∏—Ä–∞–π—Ç–µ DATA LAKE –µ—Å–ª–∏:**

- ‚úÖ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö (images, logs, JSON)
- ‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ research
- ‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –±—é–¥–∂–µ—Ç
- ‚úÖ –°–∏–ª—å–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ data engineers

### **–í—ã–±–∏—Ä–∞–π—Ç–µ DATA LAKEHOUSE –µ—Å–ª–∏:**

- ‚úÖ –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å BI –∏ AI
- ‚úÖ –ë—ã—Å—Ç—Ä–æ –º–µ–Ω—è—é—â–∏–µ—Å—è –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- ‚úÖ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ workload-—ã (SQL, ML, Streaming)
- ‚úÖ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–∫–≤–æ–∑–Ω–æ–º—É data governance

---

## üîß **–ú–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏**

### **DWH ‚Üí Lakehouse (–ü–ª–∞–≤–Ω–∞—è –º–∏–≥—Ä–∞—Ü–∏—è)**

```sql
-- –§–∞–∑–∞ 1: Data Mirroring
COPY fact_sales TO 's3://lakehouse/mirror/sales/'
FORMAT PARQUET;

-- –§–∞–∑–∞ 2: Hybrid Querying
-- –ó–∞–ø—Ä–æ—Å—ã —Ä–∞–±–æ—Ç–∞—é—Ç —Å –æ–±–æ–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
CREATE VIEW unified_sales AS
SELECT * FROM legacy_dwh.fact_sales
UNION ALL
SELECT * FROM delta.`s3://lakehouse/mirror/sales/`;

-- –§–∞–∑–∞ 3: Cut-over
-- –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ workload-–æ–≤ –Ω–∞ Lakehouse
```

### **Data Lake ‚Üí Lakehouse (–≠–≤–æ–ª—é—Ü–∏—è)**

```csharp
// –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è raw data –≤ Delta/Iceberg
// –î–æ: Raw files
var rawDf = spark.Read().Parquet("s3://datalake/raw/sales/");

// –ü–æ—Å–ª–µ: Delta Lake with ACID
rawDf.Write().Format("delta")
    .Option("delta.autoOptimize.optimizeWrite", "true")
    .Save("s3://lakehouse/sales/");
```

---

## üí∞ **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ TCO (Total Cost of Ownership)**

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç      | Traditional DWH | Data Lake | Data Lakehouse |
| -------------- | --------------- | --------- | -------------- |
| **Storage**    | $$$$            | $         | $$             |
| **Compute**    | $$$$            | $$        | $$$            |
| **ETL/ELT**    | $$$             | $$        | $$             |
| **ML/AI**      | $$$$            | $$        | $$             |
| **Management** | $$$             | $$        | $              |
| **–ò—Ç–æ–≥–æ TCO**  | **$$$$**        | **$$**    | **$$$**        |

---

## üè¢ **–†–µ–∞–ª—å–Ω—ã–µ –∫–µ–π—Å—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è**

### **Financial Services - Lakehouse**

```sql
-- –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ ML-–º–æ–¥–µ–ª–µ–π
-- –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
SELECT account_id, transaction_amount,
       fraud_prediction_score
FROM delta.`s3://lakehouse/transactions/`
WHERE fraud_prediction_score > 0.8;

-- –õ–æ–≥–∏ –∏ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (–ø–æ–ª—É—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
SELECT user_id, navigation_pattern,
       risk_assessment_model(behavior_data) as risk_score
FROM delta.`s3://lakehouse/user_behavior/`
```

### **E-commerce - Hybrid Approach**

```yaml
Architecture:
  Real-time Analytics: Traditional DWH (Snowflake)
  Customer Analytics: Lakehouse (Databricks)
  ML Recommendations: Lakehouse ML Runtime
  Log Analysis: Data Lake (S3 + Spark)
```

### **Healthcare - Data Lakehouse**

```csharp
// –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ structured –∏ unstructured –¥–∞–Ω–Ω—ã—Ö
// EHR –¥–∞–Ω–Ω—ã–µ
var ehrData = spark.Read().Table("delta.`s3://lakehouse/ehr/`");

// –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
var imagesDf = spark.Read().Format("binaryFile")
    .Load("s3://lakehouse/medical_images/");

// ML pipeline –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
var diagnosisModel = healthcareMlPipeline.Fit(ehrData);
var predictions = diagnosisModel.Transform(imagesDf);
```

---

## üöÄ **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: Benchmark**

| Workload             | Traditional DWH | Data Lake | Data Lakehouse |
| -------------------- | --------------- | --------- | -------------- |
| **SQL Analytics**    | 100%            | 40%       | 90%            |
| **ML Training**      | 30%             | 100%      | 95%            |
| **Data Ingestion**   | 70%             | 100%      | 90%            |
| **Concurrent Users** | 80%             | 50%       | 85%            |

---

## üìà **–¢—Ä–µ–Ω–¥—ã –∏ –±—É–¥—É—â–µ–µ**

### **–ö–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π**

- Snowflake ‚Üí Snowpark (ML –≤ DWH)
- Databricks ‚Üí SQL Analytics –≤ Lakehouse
- BigQuery ‚Üí BigQuery ML + BigLake

### **Emerging Standards**

- **Delta Sharing**: Open data sharing protocol
- **Open Table Formats**: Delta, Iceberg, Hudi
- **Data Mesh**: Decentralized data ownership

---

## üö® **Anti-patterns**

1. **Lift-and-shift** DWH –≤ Lakehouse –±–µ–∑ –ø–µ—Ä–µ–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
2. **Data Swamp** –≤ Lakehouse –±–µ–∑ proper governance
3. **Over-engineering** –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö use cases
4. **Ignoring skillset** –∫–æ–º–∞–Ω–¥—ã –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

---

## ‚úÖ **Best Practices**

1. **Start with business requirements** –∞ –Ω–µ —Å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π
2. **Consider hybrid approach** –¥–ª—è –ø–ª–∞–≤–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏
3. **Implement data governance** —Å —Å–∞–º–æ–≥–æ –Ω–∞—á–∞–ª–∞
4. **Choose open formats** –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è vendor lock-in
5. **Plan for evolution** –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º

---

**–°–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑–¥–µ–ª:** [2.1 - ETL vs ELT: –ø–∞—Ä–∞–¥–∏–≥–º—ã –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö](../processes/01-etl-vs-elt-paradigms.md)
